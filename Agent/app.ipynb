{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdbe48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 759/759 [00:18<00:00, 41.28it/s]\n",
      "C:\\Users\\IDF\\AppData\\Local\\Temp\\ipykernel_23288\\3532727086.py:29: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  clean_docs.append(d.copy(update={\"page_content\": text}))\n",
      "C:\\Users\\IDF\\AppData\\Local\\Temp\\ipykernel_23288\\3532727086.py:54: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loaded existing Chroma store from chroma_db\n",
      "LLM reply:\n",
      " To set up a subnet with a custom gas fee, you need to update the `gasLimit` parameter in your subnet configuration. This parameter determines the maximum amount of gas that can be used for transactions within the subnet. You can adjust this value based on your specific requirements and network conditions.\n",
      "\n",
      "JSON updates:\n",
      " {\n",
      "  \"gasLimit\": \"10000000\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "# Configuration\n",
    "PERSIST_DIR     = \"chroma_db\"\n",
    "COLLECTION_NAME = \"avax_docs\"\n",
    "BASE_URL        = \"192.168.86.9\"\n",
    "\n",
    "# 1. Load raw MD/MDX files\n",
    "docs_path = Path(\"embeddings/raw\")                      \n",
    "loader = DirectoryLoader(\n",
    "    str(docs_path),\n",
    "    glob=\"**/*.md*\",                         \n",
    "    loader_cls=UnstructuredMarkdownLoader,\n",
    "    show_progress=True,\n",
    ")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "\n",
    "# %%\n",
    "import re\n",
    "# 2. Clean out images and front-matter\n",
    "clean_docs = []\n",
    "for d in raw_docs:\n",
    "    text = re.sub(r\"!\\[.*?\\]\\(.*?\\)\", \"\", d.page_content)      # remove images\n",
    "    text = re.sub(r\"^---.*?---\\s*\", \"\", text, flags=re.S)     # drop YAML front-matter\n",
    "    clean_docs.append(d.copy(update={\"page_content\": text}))\n",
    "\n",
    "\n",
    "# %%\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# 3. Chunk into ~1,500-char pieces\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=200,\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "docs = splitter.split_documents(clean_docs)\n",
    "\n",
    "\n",
    "# %%\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 4. Prepare embeddings + vector store (idempotent)\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:v1.5\",\n",
    "    base_url=BASE_URL, \n",
    ")\n",
    "\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "    # Reload existing store\n",
    "    vectordb = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    print(f\"[+] Loaded existing Chroma store from {PERSIST_DIR}\")\n",
    "else:\n",
    "    # Create new store, ingest, and persist\n",
    "    vectordb = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    vectordb.add_documents(docs)\n",
    "    vectordb.persist()\n",
    "    print(f\"[+] Created new Chroma store and saved to {PERSIST_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# 5. Define response schema and parser\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"reply\", description=\"Conversational answer.\"),\n",
    "    ResponseSchema(\n",
    "        name=\"update\",\n",
    "        description=\"JSON merge-patch for the config (empty if no change).\",\n",
    "    ),\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "json_structure = '''{\n",
    " \"subnetId\":\"string,\n",
    " \"vmId\": \"string\",\n",
    "  \"evmChainId\": \"number\",\n",
    "  \"gasLimit\": \"number\", \n",
    "  \"targetBlockRate\": \"number\",\n",
    "  \"tokenAllocations\": [\n",
    "    {\n",
    "      \"address\": \"string\",\n",
    "      \"amount\": \"string\"\n",
    "    }\n",
    "  ],\n",
    "  \"feeConfig\": {\n",
    "    \"minBaseFee\": \"string\",\n",
    "    \"baseFeeChangeDenominator\": \"number\",\n",
    "    \"minBlockGasCost\": \"string\", \n",
    "    \"maxBlockGasCost\": \"string\",\n",
    "    \"blockGasCostStep\": \"string\",\n",
    "    \"targetGas\": \"string\"\n",
    "  },\n",
    "  \"contractDeployerAllowListConfig\": {\n",
    "    \"enabled\": \"boolean\",\n",
    "    \"admins\": [\"string\"],\n",
    "    \"members\": [\"string\"],\n",
    "    \"enabledAddresses\": [\"string\"]\n",
    "  },\n",
    "  \"contractNativeMinterConfig\": {\n",
    "    \"enabled\": \"boolean\", \n",
    "    \"admins\": [\"string\"],\n",
    "    \"members\": [\"string\"],\n",
    "    \"enabledAddresses\": [\"string\"],\n",
    "  },\n",
    "  \"txAllowListConfig\": {\n",
    "    \"enabled\": \"boolean\",\n",
    "    \"admins\": [\"string\"], \n",
    "    \"members\": [\"string\"],\n",
    "    \"enabledAddresses\": [\"string\"]\n",
    "  },\n",
    "  \"feeManagerEnabled\": \"boolean\",\n",
    "  \"feeManagerAdmins\": [\"string\"],\n",
    "  \"rewardManagerEnabled\": \"boolean\", \n",
    "  \"rewardManagerAdmins\": [\"string\"]\n",
    "}'''\n",
    "\n",
    "SYSTEM_TEMPLATE = (\n",
    "    \"You are Avalanche-GPT, an assistant that answers developer questions about Avalanche infrastructure \"\n",
    "    \"and helps them edit their Avalanche network configuration.\\n\\n\"\n",
    "    \"You output exactly two things in JSON: reply (a human-friendly answer) and update \"\n",
    "    \"(a JSON merge-patch for the config; empty object if no change is needed).\\n\\n\"\n",
    "    \"Allowed JSON fields/types:\\n{json_structure}\\n\\n\"\n",
    "    \"⚠️ OUTPUT FORMAT:\\nRespond **only** with a JSON object containing reply and update.\\n\"\n",
    "    \"{format_instructions}\"\n",
    ")\n",
    "system_msg = SystemMessagePromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "\n",
    "# 8. Human prompt with context, config, and question\n",
    "HUMAN_TEMPLATE = (\n",
    "    \"Context from docs:\\n{context}\\n\\n\"\n",
    "    \"Question:\\n{question}\"    # <-- must be {query}, not {question}\n",
    ")\n",
    "human_msg = HumanMessagePromptTemplate.from_template(HUMAN_TEMPLATE)\n",
    "\n",
    "# 9. Assemble chat prompt and inject static pieces\n",
    "prompt = (\n",
    "    ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "    .partial(\n",
    "        format_instructions=format_instructions,\n",
    "        json_structure=json_structure,\n",
    "    )\n",
    ")\n",
    "\n",
    "# 10. LLM & RetrievalQA chain setup\n",
    "llm = OllamaLLM(\n",
    "    model=\"qwen2.5-coder:3b-instruct-q8_0\",\n",
    "    temperature=0.2,\n",
    "    base_url=BASE_URL,\n",
    "    num_ctx=16384,\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"chat_history\": \"User: Hi\\nBot: Hello! How can I help?\\n\",\n",
    "    \"user_config\":  json.dumps({\"gasLimit\": \"8000000\"}),\n",
    "    \"question\":     \"How do I set up a subnet with a custom gas fee?\"\n",
    "}\n",
    "\n",
    "# Combine the three into the single string RetrievalQA expects under 'query'\n",
    "query_string = (\n",
    "    f\"Previous chat history:\\n{payload['chat_history']}\\n\\n\"\n",
    "    f\"Current user config JSON:\\n{payload['user_config']}\\n\\n\"\n",
    "    f\"Current question:\\n{payload['question']}\"\n",
    ")\n",
    "\n",
    "result_text = qa_chain.invoke({\"query\": query_string})[\"result\"]\n",
    "parsed      = parser.parse(result_text)\n",
    "\n",
    "print(\"LLM reply:\\n\", parsed[\"reply\"])\n",
    "print(\"\\nJSON updates:\\n\", json.dumps(parsed[\"update\"], indent=2))\n",
    "\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d216cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
